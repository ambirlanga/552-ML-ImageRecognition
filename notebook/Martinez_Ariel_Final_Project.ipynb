{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d6e6c8",
   "metadata": {},
   "source": [
    "<center><h1>Martinez_Ariel_Final_Project</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6ef793",
   "metadata": {},
   "source": [
    "Name: Ariel Martinez Birlanga\n",
    "<br>\n",
    "Github Username: ambirlanga\n",
    "<br>\n",
    "USC ID: 5483611649"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9056dc7",
   "metadata": {},
   "source": [
    "## Project Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00fdf71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and Keras \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50, ResNet101, EfficientNetB0, VGG16\n",
    "from tensorflow.keras.layers import Activation, Input, Dense, Dropout, Flatten, BatchNormalization, GlobalAveragePooling2D, Rescaling, Resizing, RandomCrop, RandomTranslation, RandomRotation, RandomZoom, RandomFlip, RandomContrast\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Sklearn \n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Utility \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR) # Remove INFO messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b2bc41",
   "metadata": {},
   "source": [
    "## Data Exploration and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42279d91",
   "metadata": {},
   "source": [
    "First we create all the dataset with a 20% cross validation split and one-hot encoding. We also stablish basic parameters like Image size and batch. Seed 42 is used for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d79bacde",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory ../Data/seg_train",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m \u001b[38;5;66;03m# Good balance\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Training dataset\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Resize \u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Batch \u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcategorical\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# One-hot encoding \u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 20% validation\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Subset\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m  \u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Validation dataset\u001b[39;00m\n\u001b[0;32m     21\u001b[0m val \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[0;32m     22\u001b[0m     train_dir,\n\u001b[0;32m     23\u001b[0m     image_size\u001b[38;5;241m=\u001b[39mIMG_SIZE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     29\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\USC-Intro\\lib\\site-packages\\keras\\src\\utils\\image_dataset.py:210\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[1;32m--> 210\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALLOWLIST_FORMATS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\USC-Intro\\lib\\site-packages\\keras\\src\\utils\\dataset_utils.py:542\u001b[0m, in \u001b[0;36mindex_directory\u001b[1;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    541\u001b[0m     subdirs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    544\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\USC-Intro\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:768\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \n\u001b[0;32m    755\u001b[0m \u001b[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(path):\n\u001b[1;32m--> 768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[0;32m    769\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    770\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    771\u001b[0m       message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[0;32m    773\u001b[0m \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    776\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str_any(filename)\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mGetChildren(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[0;32m    778\u001b[0m ]\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Could not find directory ../Data/seg_train"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "train_dir =  '../Data/seg_train'\n",
    "test_dir =  '../Data/seg_test'\n",
    "\n",
    "# Parameters\n",
    "IMG_SIZE = (224, 224) \n",
    "BATCH_SIZE = 32 # Good balance\n",
    "\n",
    "# Training dataset\n",
    "train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=IMG_SIZE,  # Resize \n",
    "    batch_size=BATCH_SIZE,  # Batch \n",
    "    label_mode=\"categorical\",  # One-hot encoding \n",
    "    validation_split=0.2,  # 20% validation\n",
    "    subset=\"training\",  # Subset\n",
    "    seed=42  \n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "val = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"categorical\",\n",
    "    validation_split=0.2,  \n",
    "    subset=\"validation\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Test dataset\n",
    "test = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode=\"categorical\" \n",
    ")\n",
    "\n",
    "print(\"\")\n",
    "classes = train.class_names\n",
    "num_classes = len(classes)\n",
    "print(f\"Class names: {classes}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "print(\"\\nOne-hot encoding Test\")\n",
    "for images, labels in train.take(1):  \n",
    "    print(\"Sample:\", labels[0]) \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203fdee1",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33001724",
   "metadata": {},
   "source": [
    "We apply sevelar augmentation techniques sequentialy (with Keras):\n",
    "    \n",
    "    Random cropping\n",
    "    Random translation\n",
    "    Random Rotation\n",
    "    Random Flipping\n",
    "    Brightness\n",
    "    Random Contrast \n",
    "    Random Zoom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccfe6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation pipeline\n",
    "pipeline = Sequential(\n",
    "  [    \n",
    "        # Croping gives considerably bad results\n",
    "        #RandomCrop(height=200, width=200), # Crop and resize\n",
    "        #Resizing(224, 224),    \n",
    "        RandomFlip(\"horizontal\"), # Flip H\n",
    "        RandomRotation(factor=0.05), # Rotations \n",
    "        RandomZoom(height_factor=(-0.1, 0.1), width_factor=(-0.1, 0.1)), # Zoom \n",
    "        RandomTranslation(height_factor=0.1, width_factor=0.1), # Translations\n",
    "        RandomContrast(factor=0.1), # Contrast \n",
    "        tf.keras.layers.Lambda(lambda x: tf.image.random_brightness(x, max_delta=0.1)), # Brightness\n",
    "    ], \n",
    "    name = \"pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81d0bb",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbcbfcf",
   "metadata": {},
   "source": [
    "We create a function campable of creating a \"Frozen\" model for the 4 targeted types, 256 units, ReLU activation, L2 Regularization, Batch normalization, softmax output, and 20% dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24af55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation\n",
    "\n",
    "model_mapping = {\n",
    "    \"EfficientNetB0\": EfficientNetB0,\n",
    "    \"VGG16\": VGG16,\n",
    "    \"ResNet50\": ResNet50,\n",
    "    \"ResNet101\": ResNet101\n",
    "}\n",
    "\n",
    "def create_model(model_type, input_shape):\n",
    "    inputs = Input(shape = input_shape)\n",
    "    x = pipeline(inputs)\n",
    "    basic_model = model_mapping[model_type](weights=\"imagenet\", include_top=False, input_tensor = x)\n",
    "\n",
    "    # Freeze \n",
    "    basic_model.trainable = False\n",
    "\n",
    "    # Head\n",
    "    x = GlobalAveragePooling2D(name = \"avg_pool\")(basic_model.output) \n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)    \n",
    "    outputs = Dense(6, activation = \"softmax\", kernel_regularizer=l2(0.01))(x)\n",
    "    return Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d107c2",
   "metadata": {},
   "source": [
    "##  Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc27f5e8",
   "metadata": {},
   "source": [
    "We train the models using multinomial cross entropy as well as ADAM optimizer. Epoch = 100 as default. Use of learning rate scheduler. Early stopping at 10 instances without improvement,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a5f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, test_data, epochs, name, lr=0.001):\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "        \n",
    "    # Set model\n",
    "    model.compile(\n",
    "        optimizer=Adam(lr), # ADAM\n",
    "        loss=\"categorical_crossentropy\", # Multinomial cross entropy\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    # Model checkpoint callback\n",
    "    checkpoint_filepath = f'../ModelCheckpoints/{name}.h5'\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,  \n",
    "        monitor='val_loss', \n",
    "        save_best_only=True,  \n",
    "        save_weights_only=False\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        validation_data=val_data,\n",
    "        epochs=epochs,\n",
    "        callbacks=[lr_schedule, tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)] # Early stopping\n",
    "    )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9ffbb1",
   "metadata": {},
   "source": [
    "### ResNet50 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c990cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (224, 224, 3)\n",
    "histories = {}\n",
    "models ={}\n",
    "best ={}\n",
    "\n",
    "print(\"Training ResNet50\")\n",
    "models[\"ResNet50\"]  = create_model(\"ResNet50\", INPUT_SHAPE)\n",
    "histories[\"ResNet50\"],best[\"ResNet50\"]  = train_model(models[\"ResNet50\"], train, val, test, 1, \"ResNet50\", 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b5bb4",
   "metadata": {},
   "source": [
    "### ResNet101 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c71c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training ResNet101\")\n",
    "models[\"ResNet101\"]  = create_model(\"ResNet101\", INPUT_SHAPE)\n",
    "histories[\"ResNet101\"], best[\"ResNet101\"]= train_model(models[\"ResNet101\"], train, val, test, 70, \"ResNet101\", 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a5a5a8",
   "metadata": {},
   "source": [
    "### VGG16 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c84423",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training VGG16\")\n",
    "models[\"VGG16\"] = create_model(\"VGG16\", INPUT_SHAPE)\n",
    "histories[\"VGG16\"], best[\"VGG16\"] = train_model(models[\"VGG16\"], train, val, test, 70, \"VGG16\", 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be5c944",
   "metadata": {},
   "source": [
    "### EfficientNetB0 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342dc469",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training EfficientNetB0:\")\n",
    "models[\"EfficientNetB0\"] = create_model(\"EfficientNetB0\", INPUT_SHAPE)\n",
    "histories[\"EfficientNetB0\"], best[\"EfficientNetB0\"] = train_model(models[\"EfficientNetB0\"], train, val, test, 70, \"EfficientNetB0\", 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd513779",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Charts and scores for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52400997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, train_data, val_data, test_data, history):\n",
    "    def calculate_metrics(model, dataset, name):\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        for x, y in dataset:\n",
    "            # Collect probabilities (predicted scores) and true labels\n",
    "            probs = model.predict(x, verbose=0)  # Predicted probabilities\n",
    "            predictions.extend(probs)  # Add probabilities\n",
    "            true_labels.extend(y.numpy())  # Add one-hot encoded labels\n",
    "\n",
    "        # Convert to NumPy arrays\n",
    "        predictions = np.array(predictions)  # Shape: (num_samples, num_classes)\n",
    "        true_labels = np.array(true_labels)  # Shape: (num_samples, num_classes)\n",
    "\n",
    "        # Classification report using argmax\n",
    "        pred_labels = np.argmax(predictions, axis=-1)\n",
    "        true_labels_argmax = np.argmax(true_labels, axis=-1)\n",
    "        report = classification_report(true_labels_argmax, pred_labels, target_names=classes, output_dict=True)\n",
    "        print(f\"{name} Metrics:\\n\", classification_report(true_labels_argmax, pred_labels, target_names=classes))\n",
    "\n",
    "        # ROC AUC score (macro or weighted)\n",
    "        roc_auc = roc_auc_score(true_labels, predictions, multi_class=\"ovr\", average=\"macro\")\n",
    "        print(f\"{name} ROC AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "        return report, roc_auc\n",
    "\n",
    "    # Plot training vs validation loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Evaluate metrics for each dataset\n",
    "    print(\"Evaluating Training Data...\")\n",
    "    train_report, train_auc = calculate_metrics(model, train_data, \"Training\")\n",
    "    print(\"\\nEvaluating Validation Data...\")\n",
    "    val_report, val_auc = calculate_metrics(model, val_data, \"Validation\")\n",
    "    print(\"\\nEvaluating Test Data...\")\n",
    "    test_report, test_auc = calculate_metrics(model, test_data, \"Test\")\n",
    "\n",
    "\n",
    "\n",
    "    # Optionally return metrics if needed for further analysis\n",
    "    return {\n",
    "        \"Train\": {\"report\": train_report, \"auc\": train_auc},\n",
    "        \"Validation\": {\"report\": val_report, \"auc\": val_auc},\n",
    "        \"Test\": {\"report\": test_report, \"auc\": test_auc},\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb1492",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_summary = []\n",
    "for model_type in  [\"ResNet50\",\"ResNet101\",\"VGG16\",\"EfficientNetB0\"]:\n",
    "    print(f\"Evaluation {model_type}:\")\n",
    "    results = eval_model(best[model_type], train, val, test, histories[model_type])\n",
    "    \n",
    "     # Extract metrics for each dataset\n",
    "    for dataset_name, metrics in results.items():\n",
    "        report = metrics[\"report\"][\"weighted avg\"]\n",
    "        auc = metrics[\"auc\"]\n",
    "        \n",
    "        # Append metrics to summary list\n",
    "        metrics_summary.append({\n",
    "            \"Model\": model_type,\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Precision\": report[\"precision\"],\n",
    "            \"Recall\": report[\"recall\"],\n",
    "            \"F1 Score\": report[\"f1-score\"],\n",
    "            \"AUC\": auc,\n",
    "        })\n",
    "    print(\"\\n\\n\\n\")\n",
    "metrics_df = pd.DataFrame(metrics_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the DataFrame\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295a6f9a",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "AA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c41e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
